{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy operates on a pipeline which consists of many steps including tokenisation, lemmisation and other optional add in components.\n",
    "\n",
    "Pipeline is instantiated with a spacy pipeline by loading either a blank one with `spacy.blank(<language>)` or with `spacy.load(<name of trained model>)`\n",
    "\n",
    "Objects\n",
    "- nlp : pipeline\n",
    "- doc : the end result of the raw text through the pipeline\n",
    "- token : individual token (word or component) in the document\n",
    "- Span : Mulitple tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_string = \"Randy is learning NLP on spacy!\"\n",
    "doc =nlp(sample_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Randy is learning NLP on spacy!'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens\n",
    "tokens = [tok for tok in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Randy, is, learning, NLP, on, spacy, !]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Randy, is, learning]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spans\n",
    "sample_span = tokens[:3]\n",
    "sample_span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examining Randy in idx 0\n",
      "Alpha: True\n",
      "ascii: True\n",
      "Title case: True\n",
      "Lower case: False\n",
      "Left punctuation: False\n",
      "Right punctuation: False\n",
      "Number: False\n",
      "OOV?: True\n",
      "url: False\n",
      "Email: False\n",
      "Examining is in idx 1\n",
      "Alpha: True\n",
      "ascii: True\n",
      "Title case: False\n",
      "Lower case: True\n",
      "Left punctuation: False\n",
      "Right punctuation: False\n",
      "Number: False\n",
      "OOV?: True\n",
      "url: False\n",
      "Email: False\n",
      "Examining learning in idx 2\n",
      "Alpha: True\n",
      "ascii: True\n",
      "Title case: False\n",
      "Lower case: True\n",
      "Left punctuation: False\n",
      "Right punctuation: False\n",
      "Number: False\n",
      "OOV?: True\n",
      "url: False\n",
      "Email: False\n",
      "Examining NLP in idx 3\n",
      "Alpha: True\n",
      "ascii: True\n",
      "Title case: False\n",
      "Lower case: False\n",
      "Left punctuation: False\n",
      "Right punctuation: False\n",
      "Number: False\n",
      "OOV?: True\n",
      "url: False\n",
      "Email: False\n",
      "Examining on in idx 4\n",
      "Alpha: True\n",
      "ascii: True\n",
      "Title case: False\n",
      "Lower case: True\n",
      "Left punctuation: False\n",
      "Right punctuation: False\n",
      "Number: False\n",
      "OOV?: True\n",
      "url: False\n",
      "Email: False\n",
      "Examining spacy in idx 5\n",
      "Alpha: True\n",
      "ascii: True\n",
      "Title case: False\n",
      "Lower case: True\n",
      "Left punctuation: False\n",
      "Right punctuation: False\n",
      "Number: False\n",
      "OOV?: True\n",
      "url: False\n",
      "Email: False\n",
      "Examining ! in idx 6\n",
      "Alpha: False\n",
      "ascii: True\n",
      "Title case: False\n",
      "Lower case: False\n",
      "Left punctuation: False\n",
      "Right punctuation: False\n",
      "Number: False\n",
      "OOV?: True\n",
      "url: False\n",
      "Email: False\n"
     ]
    }
   ],
   "source": [
    "# Lexical attributes\n",
    "# Doc: https://spacy.io/api/token\n",
    "# e.g token.is_alpha , token.is_punct , token.like_num\n",
    "for idx, token in enumerate(tokens):\n",
    "    print(f\"Examining {token} in idx {idx}\")\n",
    "    print(f\"Alpha: {token.is_alpha}\")\n",
    "    print(f\"ascii: {token.is_ascii}\")\n",
    "    print(f\"Title case: {token.is_title}\")\n",
    "    print(f\"Lower case: {token.is_lower}\")\n",
    "    print(f\"Left punctuation: {token.is_left_punct}\")\n",
    "    print(f\"Right punctuation: {token.is_right_punct}\")\n",
    "    print(f\"Number: {token.like_num}\")\n",
    "    print(f\"OOV?: {token.is_oov}\")\n",
    "    print(f\"url: {token.like_url}\")\n",
    "    print(f\"Email: {token.like_email}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\tools\\miniconda3\\envs\\nlp-tutorials\\lib\\site-packages\\spacy\\util.py:877: UserWarning: [W095] Model 'en_core_web_sm' (3.3.0) was trained with spaCy v3.3 and may not be 100% compatible with the current version (3.4.3). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "nlp_en_core_web_sm = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[chngyuanlong@gmail.com]\n",
      "Like email : True\n"
     ]
    }
   ],
   "source": [
    "sample_email = \"chngyuanlong@gmail.com\"\n",
    "sample_url = \"https://myfirstdatasciencejob.wordpress.com/\"\n",
    "doc = nlp_en_core_web_sm(sample_email)\n",
    "tokens = [tok for tok in doc]\n",
    "print(tokens)\n",
    "print(f\"Like email : {tokens[0].like_email}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[https://myfirstdatasciencejob.wordpress.com/]\n",
      "Like url : True\n"
     ]
    }
   ],
   "source": [
    "doc = nlp_en_core_web_sm(sample_url)\n",
    "tokens = [tok for tok in doc]\n",
    "print(tokens)\n",
    "print(f\"Like url : {tokens[0].like_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[My, cat, is, like, a, god, ., Oh, opps, I, meant, dog]\n"
     ]
    }
   ],
   "source": [
    "sample_similarity_sentence = \"My cat is like a god. Oh opps I meant dog\"\n",
    "doc = nlp_en_core_web_sm(sample_similarity_sentence)\n",
    "tokens = [tok for tok in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34402838349342346\n",
      "0.23464104533195496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Randy\\AppData\\Local\\Temp\\ipykernel_24384\\2458266744.py:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  print(tokens[1].similarity(tokens[5]))\n",
      "C:\\Users\\Randy\\AppData\\Local\\Temp\\ipykernel_24384\\2458266744.py:2: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  print(tokens[1].similarity(tokens[-1]))\n"
     ]
    }
   ],
   "source": [
    "print(tokens[1].similarity(tokens[5]))\n",
    "print(tokens[1].similarity(tokens[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The thing that fascinates me is that some inbuilt features that relates to core NLP tasks that dependency parsing, NER, POS, identification of syntactic heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: My, POS Tag: PRON, Dep label: poss, Head token: cat, Lemmatised token: my\n",
      "Text: cat, POS Tag: NOUN, Dep label: nsubj, Head token: is, Lemmatised token: cat\n",
      "Text: is, POS Tag: AUX, Dep label: ROOT, Head token: is, Lemmatised token: be\n",
      "Text: like, POS Tag: ADP, Dep label: prep, Head token: is, Lemmatised token: like\n",
      "Text: a, POS Tag: DET, Dep label: det, Head token: god, Lemmatised token: a\n",
      "Text: god, POS Tag: NOUN, Dep label: pobj, Head token: like, Lemmatised token: god\n",
      "Text: ., POS Tag: PUNCT, Dep label: punct, Head token: is, Lemmatised token: .\n",
      "Text: Oh, POS Tag: INTJ, Dep label: poss, Head token: opps, Lemmatised token: oh\n",
      "Text: opps, POS Tag: NOUN, Dep label: npadvmod, Head token: meant, Lemmatised token: opps\n",
      "Text: I, POS Tag: PRON, Dep label: nsubj, Head token: meant, Lemmatised token: I\n",
      "Text: meant, POS Tag: VERB, Dep label: ROOT, Head token: meant, Lemmatised token: mean\n",
      "Text: dog, POS Tag: NOUN, Dep label: dobj, Head token: meant, Lemmatised token: dog\n"
     ]
    }
   ],
   "source": [
    "for token in tokens:\n",
    "    print(f\"Text: {token.text}, POS Tag: {token.pos_}, Dep label: {token.dep_}, Head token: {token.head.text}, Lemmatised token: {token.lemma_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there is no entity it will not print it.\n",
    "for ent in doc.ents:\n",
    "    # Print the entity text and label\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[We, will, need, to, destroy, the, Whiskey, company, else, they, get, too, big]\n"
     ]
    }
   ],
   "source": [
    "sample_NER = \"We will need to destroy the Whiskey company else they get too big\"\n",
    "doc = nlp_en_core_web_sm(sample_NER)\n",
    "tokens = [tok for tok in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whiskey ORG\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    # Print the entity text and label\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[From website](https://course.spacy.io/en/chapter1)\n",
    "\n",
    "Compared to regular expressions, the matcher works with Doc and Token objects instead of only strings.\n",
    "\n",
    "It's also more flexible: you can search for texts but also other lexical attributes.\n",
    "\n",
    "You can even write rules that use a model's predictions.\n",
    "\n",
    "For example, find the word \"duck\" only if it's a verb, not a noun."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! Important things to note: each dictionary pattern represents one token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_matcher_text = \"I need two dozen eggs and maybe 1kg of minced beef. Do not beef me\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp_en_core_web_sm(sample_matcher_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I PRON\n",
      "need VERB\n",
      "two NUM\n",
      "dozen NOUN\n",
      "eggs NOUN\n",
      "and CCONJ\n",
      "maybe ADV\n",
      "1 NUM\n",
      "kg NOUN\n",
      "of ADP\n",
      "minced VERB\n",
      "beef NOUN\n",
      ". PUNCT\n",
      "Do AUX\n",
      "not PART\n",
      "beef VERB\n",
      "me PRON\n"
     ]
    }
   ],
   "source": [
    "tokens = [tok for tok in doc]\n",
    "\n",
    "for token in tokens:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Matcher with the shared vocabulary\n",
    "matcher = Matcher(nlp_en_core_web_sm.vocab)\n",
    "\n",
    "# Let's try to catch the following pattern : VERB , [NOUN or PRON]\n",
    "VERB_PATTERN = [{\"POS\":\"VERB\"}, {\"POS\":{\"IN\":[\"PRON\", \"NOUN\"]}}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: ['minced beef', 'beef me']\n"
     ]
    }
   ],
   "source": [
    "# Add the pattern to the matcher\n",
    "matcher.add(\"items_pattern\", [VERB_PATTERN])\n",
    "\n",
    "# Use the matcher on the doc\n",
    "matches = matcher(doc)\n",
    "print(\"Matches:\", [doc[start:end].text for match_id, start, end in matches])\n",
    "\n",
    "# It fetches according to the POS tags but minced meat would be more like ADJ NOUN rather than VERB NOUN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StringStore, Lexemes and Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "doc = nlp(\"I have a cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_hash = nlp.vocab.strings['cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5439657043933447811"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_string = nlp.vocab.strings[cat_hash]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cat'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14692702688101715474"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab.strings['have']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'have'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab.strings[14692702688101715474]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3687079329867984377"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab.strings['fishy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"[E018] Can't retrieve string for hash '3687079329867984377'. This usually refers to an issue with the `Vocab` or `StringStore`.\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m nlp\u001b[39m.\u001b[39;49mvocab\u001b[39m.\u001b[39;49mstrings[\u001b[39m3687079329867984377\u001b[39;49m]\n",
      "File \u001b[1;32mc:\\tools\\miniconda3\\envs\\nlp-tutorials\\lib\\site-packages\\spacy\\strings.pyx:159\u001b[0m, in \u001b[0;36mspacy.strings.StringStore.__getitem__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"[E018] Can't retrieve string for hash '3687079329867984377'. This usually refers to an issue with the `Vocab` or `StringStore`.\""
     ]
    }
   ],
   "source": [
    "nlp.vocab.strings[3687079329867984377]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3687079329867984377"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab.strings.add('fishy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3687079329867984377"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab.strings['fishy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fishy'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab.strings[3687079329867984377]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexeme = nlp.vocab['fishy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3687079329867984377"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexeme.orth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Doc, Span manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc\n",
    "\n",
    "words = [\"spacy\",\"is\",'cool','!']\n",
    "spaces = [True, True, False, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spacy is cool!'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Doc, Span and Entities manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "words = [\"I\", \"like\", \"David\", \"Bowie\"]\n",
    "spaces = [True, True, True, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = Doc(nlp.vocab, words, spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I like David Bowie'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "span = Span(doc, 2,4, label=\"PERSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'David Bowie'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "380"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.ents = [span]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('David Bowie', 'PERSON')]\n"
     ]
    }
   ],
   "source": [
    "# Print entities' text and labels\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the following code that best use the features of Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\tools\\miniconda3\\envs\\nlp-tutorials\\lib\\site-packages\\spacy\\util.py:877: UserWarning: [W095] Model 'en_core_web_sm' (3.3.0) was trained with spaCy v3.3 and may not be 100% compatible with the current version (3.4.3). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found proper noun before a verb: Berlin\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Berlin looks like a nice city\")\n",
    "\n",
    "# Get all tokens and part-of-speech tags\n",
    "token_texts = [token.text for token in doc]\n",
    "pos_tags = [token.pos_ for token in doc]\n",
    "\n",
    "for index, pos in enumerate(pos_tags):\n",
    "    # Check if the current token is a proper noun\n",
    "    if pos == \"PROPN\":\n",
    "        # Check if the next token is a verb\n",
    "        if pos_tags[index + 1] == \"VERB\":\n",
    "            result = token_texts[index]\n",
    "            print(\"Found proper noun before a verb:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\tools\\miniconda3\\envs\\nlp-tutorials\\lib\\site-packages\\spacy\\util.py:877: UserWarning: [W095] Model 'en_core_web_sm' (3.3.0) was trained with spaCy v3.3 and may not be 100% compatible with the current version (3.4.3). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Singapore takes the cake everytime!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found proper noun before a verb: Singapore\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    if (token.pos_ == \"PROPN\") and doc[token.i + 1].pos_ == \"VERB\":\n",
    "        res = token.text\n",
    "        print(\"Found proper noun before a verb:\", res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarity Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\tools\\miniconda3\\envs\\nlp-tutorials\\lib\\site-packages\\spacy\\util.py:877: UserWarning: [W095] Model 'en_core_web_md' (3.3.0) was trained with spaCy v3.3 and may not be 100% compatible with the current version (3.4.3). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Fee fi fo fum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "fum_vector = doc[3].vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.2737e-01, -3.4660e-01,  2.7209e-01,  3.0464e-01,  3.2837e-01,\n",
       "       -1.7831e-02,  1.1909e+00, -1.0056e-01,  2.2841e-01, -1.5137e+00,\n",
       "        5.0329e-01,  1.7327e-01,  5.1083e-01,  1.7106e-01, -3.7955e-01,\n",
       "       -2.7223e-01,  4.1014e-01, -1.6467e+00,  2.7785e-01, -2.9190e-01,\n",
       "        8.9143e-02,  4.7478e-01,  2.1643e-01,  4.0249e-01,  5.0436e-02,\n",
       "       -7.3996e-02,  3.0738e-01, -3.6356e-01,  5.9862e-01, -4.0765e-01,\n",
       "        3.8553e-01, -3.0245e-01, -3.1639e-01, -1.5023e-01, -1.0749e-01,\n",
       "       -6.5798e-01,  1.6014e-02,  2.9421e-01,  1.1150e+00,  6.6496e-01,\n",
       "       -6.7577e-01, -5.0856e-01, -7.1808e-01, -3.0598e-01, -4.3807e-01,\n",
       "       -7.4416e-01,  6.3013e-01, -1.1900e-01,  3.6612e-01,  1.6718e-01,\n",
       "        2.4640e-01, -1.4228e-01, -7.4555e-01, -4.5723e-01,  1.0380e-01,\n",
       "       -4.8714e-01,  2.5367e-01,  3.2114e-04,  4.7412e-01, -5.2455e-01,\n",
       "       -1.8667e-01,  3.9628e-02,  9.3008e-03,  5.5973e-02,  1.4076e-01,\n",
       "        2.6564e-02,  3.4447e-01,  9.9613e-02,  2.2072e-01, -1.9048e-01,\n",
       "       -8.2545e-02,  1.1993e+00, -1.6316e-01,  1.9302e-01,  2.2461e-01,\n",
       "       -1.0048e+00,  2.5631e-01,  1.7109e-01,  1.1325e-01, -4.9488e-01,\n",
       "        6.1004e-02,  4.7612e-01, -2.3883e-01,  2.1077e-03, -2.5718e-01,\n",
       "        3.4300e-01,  7.3740e-01, -8.3238e-01, -3.8272e-01,  1.5335e-01,\n",
       "        4.5032e-02,  2.0337e-01,  5.4698e-01,  1.3066e-01,  2.9046e-01,\n",
       "        6.9868e-01,  1.9453e-01, -3.7198e-01,  2.4603e-01, -1.6232e-02,\n",
       "        1.6005e-01,  2.8177e-01, -1.8885e-01, -5.3665e-01, -6.5316e-01,\n",
       "       -1.3196e+00,  1.2111e-01,  5.8542e-01, -8.8273e-02, -4.5669e-01,\n",
       "       -9.6698e-01,  3.8446e-01, -5.0887e-01, -1.7435e-02,  2.1227e-01,\n",
       "        3.4436e-01, -1.5647e-01, -3.6337e-01,  1.8037e-02,  3.2745e-02,\n",
       "        2.0054e-01, -1.6783e-01, -1.9564e-01, -1.0676e-01,  1.2691e-01,\n",
       "       -1.7317e-01, -8.9933e-02, -4.3911e-01,  2.3269e-01,  4.2121e-01,\n",
       "       -5.4290e-01,  7.4911e-01,  1.9058e-01, -6.4729e-01,  7.2051e-01,\n",
       "       -5.0381e-01, -2.5070e-01,  2.6164e-01, -1.2885e-02, -2.1742e-01,\n",
       "       -6.5495e-01,  3.5446e-01,  2.8909e-01,  9.9120e-02, -6.4023e-02,\n",
       "        2.6306e-01,  3.1022e-01,  3.6578e-01,  1.3751e-02, -2.4807e-01,\n",
       "       -5.1813e-01, -3.4797e-01,  2.8532e-01, -1.2933e+00, -3.3457e-01,\n",
       "        4.2044e-01,  3.5652e-01, -8.2035e-01,  1.1836e-01, -5.8168e-01,\n",
       "       -7.0048e-01, -5.8262e-01, -2.6367e-01,  3.5814e-02,  3.9759e-01,\n",
       "       -5.7962e-01,  3.7698e-01,  2.3250e-01,  2.4461e-01, -3.4268e-01,\n",
       "       -8.3464e-01,  2.8734e-01,  3.9053e-01,  6.4863e-01,  3.4301e-01,\n",
       "       -4.7204e-01, -4.8693e-01,  1.6432e-01, -2.8509e-01,  6.8891e-02,\n",
       "        8.5146e-01,  3.8613e-01,  4.0012e-01,  6.6666e-01,  9.6062e-02,\n",
       "        1.0120e+00, -3.0318e-01, -5.4014e-01,  3.7750e-01,  1.0180e-01,\n",
       "       -1.8888e-01,  1.5579e-01, -2.9817e-01, -3.2386e-02,  6.1999e-01,\n",
       "        6.2463e-01,  5.5215e-01, -5.5057e-02,  2.1647e-01, -4.7633e-01,\n",
       "       -5.5838e-02,  2.2335e-01, -2.9718e-01, -1.8607e-01,  8.4839e-02,\n",
       "       -1.2173e+00, -3.9329e-01,  3.1951e-01,  6.7136e-01, -8.2581e-01,\n",
       "       -2.9417e-01,  2.2631e-01,  1.7313e-01,  3.2470e-01,  1.5723e-01,\n",
       "        1.6194e-02, -3.1587e-02,  1.3396e-01,  7.1098e-01, -4.3789e-01,\n",
       "        3.7566e-01, -1.5595e-01,  4.4346e-02, -7.7644e-01,  1.8086e-02,\n",
       "       -8.1347e-02, -5.2356e-01,  3.2004e-01,  5.0570e-02, -5.7756e-01,\n",
       "       -5.9676e-01,  5.2174e-01,  1.6823e-01, -3.9942e-01,  1.2084e-01,\n",
       "        4.1884e-01, -5.1205e-01, -5.0036e-01, -7.4498e-01, -2.2878e-01,\n",
       "       -3.5364e-01,  5.3438e-01, -1.0996e-01, -1.4868e-01,  1.4444e-01,\n",
       "       -2.5546e-01, -3.5804e-01,  1.1819e-01, -2.2306e-01,  1.8894e-01,\n",
       "       -3.9537e-01, -4.4183e-01,  1.9611e-01,  9.1559e-01, -5.7027e-01,\n",
       "       -6.5764e-01, -1.7493e-01,  2.7273e-01, -4.1785e-01,  2.7656e-02,\n",
       "       -6.2769e-01, -2.2012e-01, -4.9916e-01, -2.9694e-01,  2.3811e-01,\n",
       "        1.2989e-01, -3.1566e-01,  1.0225e-01, -1.9930e-01,  3.3223e-02,\n",
       "       -3.9777e-01, -8.7206e-01, -2.7742e-01, -6.7748e-02,  3.7022e-01,\n",
       "       -1.6456e-02,  6.3071e-03, -4.5308e-01,  5.7079e-02, -4.1503e-01,\n",
       "        1.3346e-02, -6.7991e-01, -8.9438e-01, -7.1511e-01, -1.6845e-01,\n",
       "       -5.2627e-02, -5.0501e-02,  2.7395e-01, -1.1206e+00,  3.1446e-01,\n",
       "        1.0813e+00, -6.2657e-01, -1.0817e+00, -2.5361e-01,  6.9099e-02,\n",
       "       -2.1897e-01, -8.1935e-02,  1.7943e-01, -5.1672e-01,  6.2451e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fum_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "fee_vector = doc[0].vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.11414127796888351"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[3].similarity(doc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"This movie is the shit. I have never been blown away by such astounding storytelling in my life\")\n",
    "\n",
    "span1 = doc[0:5]\n",
    "span2 = doc[6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8902060389518738"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span1.similarity(span2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining predictions and rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\n",
    "    \"Twitch Prime, the perks program for Amazon Prime members offering free \"\n",
    "    \"loot, games and other benefits, is ditching one of its best features: \"\n",
    "    \"ad-free viewing. According to an email sent out to Amazon Prime members \"\n",
    "    \"today, ad-free viewing will no longer be included as a part of Twitch \"\n",
    "    \"Prime for new members, beginning on September 14. However, members with \"\n",
    "    \"existing annual subscriptions will be able to continue to enjoy ad-free \"\n",
    "    \"viewing until their subscription comes up for renewal. Those with \"\n",
    "    \"monthly subscriptions will have access to ad-free viewing until October 15.\"\n",
    ")\n",
    "\n",
    "# Create the match patterns\n",
    "pattern1 = [{\"LOWER\": \"amazon\"}, {\"IS_TITLE\": True, \"POS\": \"PROPN\"}]\n",
    "pattern2 = [{\"LOWER\": \"ad\"}, {\"TEXT\": \"-\"}, {\"LOWER\": \"free\"}, {\"POS\": \"NOUN\"}]\n",
    "\n",
    "# Initialize the Matcher and add the patterns\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"PATTERN1\", [pattern1])\n",
    "matcher.add(\"PATTERN2\", [pattern2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATTERN1 Amazon Prime\n",
      "PATTERN2 ad-free viewing\n",
      "PATTERN1 Amazon Prime\n",
      "PATTERN2 ad-free viewing\n",
      "PATTERN2 ad-free viewing\n",
      "PATTERN2 ad-free viewing\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Print pattern string name and text of matched span\n",
    "    print(doc.vocab.strings[match_id], doc[start:end].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = PhraseMatcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Efficient Phrase Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "\n",
    "# The next phrase is un-runable since I do not have the json,\n",
    "# but the example illustrates that if the entire listing is available\n",
    "# then using a phrase matcher would help greatly\n",
    "with open(\"exercises/en/countries.json\", encoding=\"utf8\") as f:\n",
    "    COUNTRIES = json.loads(f.read())\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "doc = nlp(\"Czech Republic may help Slovakia protect its airspace\")\n",
    "\n",
    "# Import the PhraseMatcher and initialize it\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "# Create pattern Doc objects and add them to the matcher\n",
    "# This is the faster version of: [nlp(country) for country in COUNTRIES]\n",
    "patterns = list(nlp.pipe(COUNTRIES))\n",
    "matcher.add(\"COUNTRY\", patterns)\n",
    "\n",
    "# Call the matcher on the test document and print the result\n",
    "matches = matcher(doc)\n",
    "print([doc[start:end] for match_id, start, end in matches])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "David"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [Czech Republic, Slovakia]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "import json\n",
    "\n",
    "# The next phrase is un-runable since I do not have the json,\n",
    "# but the example illustrates that if the entire listing is available\n",
    "# then using a phrase matcher would help greatly\n",
    "with open(\"exercises/en/countries.json\", encoding=\"utf8\") as f:\n",
    "    COUNTRIES = json.loads(f.read())\n",
    "with open(\"exercises/en/country_text.txt\", encoding=\"utf8\") as f:\n",
    "    TEXT = f.read()\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "patterns = list(nlp.pipe(COUNTRIES))\n",
    "matcher.add(\"COUNTRY\", patterns)\n",
    "\n",
    "# Create a doc and reset existing entities\n",
    "doc = nlp(TEXT)\n",
    "doc.ents = []\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Create a Span with the label for \"GPE\"\n",
    "    span = Span(doc, start, end, label=\"GPE\")\n",
    "\n",
    "    # Overwrite the doc.ents and add the span\n",
    "    doc.ents = list(doc.ents) + [span]\n",
    "\n",
    "    # Get the span's root head token\n",
    "    span_root_head = span.root.head\n",
    "    # Print the text of the span root's head token and the span text\n",
    "    print(span_root_head.text, \"-->\", span.text)\n",
    "\n",
    "# Print the entities in the document\n",
    "print([(ent.text, ent.label_) for ent in doc.ents if ent.label_ == \"GPE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in --> Namibia\n",
    "# in --> South Africa\n",
    "# Africa --> Cambodia\n",
    "# of --> Kuwait\n",
    "# as --> Somalia\n",
    "# Somalia --> Haiti\n",
    "# Haiti --> Mozambique\n",
    "# in --> Somalia\n",
    "# for --> Rwanda\n",
    "# Britain --> Singapore\n",
    "# War --> Sierra Leone\n",
    "# of --> Afghanistan\n",
    "# invaded --> Iraq\n",
    "# in --> Sudan\n",
    "# of --> Congo\n",
    "# earthquake --> Haiti\n",
    "# [('Namibia', 'GPE'), ('South Africa', 'GPE'), ('Cambodia', 'GPE'), ('Kuwait', 'GPE'), ('Somalia', 'GPE'), ('Haiti', 'GPE'), ('Mozambique', 'GPE'), ('Somalia', 'GPE'), ('Rwanda', 'GPE'), ('Singapore', 'GPE'), ('Sierra Leone', 'GPE'), ('Afghanistan', 'GPE'), ('Iraq', 'GPE'), ('Sudan', 'GPE'), ('Congo', 'GPE'), ('Haiti', 'GPE')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\tools\\miniconda3\\envs\\nlp-tutorials\\lib\\site-packages\\spacy\\util.py:877: UserWarning: [W095] Model 'en_core_web_sm' (3.3.0) was trained with spaCy v3.3 and may not be 100% compatible with the current version (3.4.3). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x000002310138FB80>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x0000023102406B80>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x0000023116CC89E0>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x0000023106D48A40>), ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x0000023102054240>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x0000023116CC8E40>)]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the en_core_web_sm pipeline\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Print the names of the pipeline components\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Print the full pipeline of (name, component) tuples\n",
    "print(nlp.pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['length_component', 'tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "This document is 21 tokens long.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "\n",
    "# Define the custom component\n",
    "@Language.component(\"length_component\")\n",
    "def length_component_function(doc):\n",
    "    # Get the doc's length\n",
    "    doc_length = len(doc)\n",
    "    print(f\"This document is {doc_length} tokens long.\")\n",
    "    # Return the doc\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Load the small English pipeline\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Add the component first in the pipeline and print the pipe names\n",
    "nlp.add_pipe('length_component', first=True)\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"Who let the dogs out? Who?! Who?! Who?! Who?! Who?!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complex Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, youâ€™ll be writing a custom component that uses the PhraseMatcher to find animal names in the document and adds the matched spans to the doc.ents. A PhraseMatcher with the animal patterns has already been created as the variable matcher.\n",
    "\n",
    "- Define the custom component and apply the matcher to the doc.\n",
    "- Create a Span for each match, assign the label ID for \"ANIMAL\" and overwrite the doc.ents with the new spans.\n",
    "- Add the new component to the pipeline after the \"ner\" component.\n",
    "- Process the text and print the entity text and entity label for the entities in doc.ents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\tools\\miniconda3\\envs\\nlp-tutorials\\lib\\site-packages\\spacy\\util.py:877: UserWarning: [W095] Model 'en_core_web_sm' (3.3.0) was trained with spaCy v3.3 and may not be 100% compatible with the current version (3.4.3). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "animal_patterns: [Golden Retriever, cat, turtle, Rattus norvegicus]\n",
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner', 'animal_component']\n",
      "[('cat', 'ANIMAL'), ('Golden Retriever', 'ANIMAL')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "animals = [\"Golden Retriever\", \"cat\", \"turtle\", \"Rattus norvegicus\"]\n",
    "animal_patterns = list(nlp.pipe(animals))\n",
    "print(\"animal_patterns:\", animal_patterns)\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add(\"ANIMAL\", animal_patterns)\n",
    "\n",
    "# Define the custom component\n",
    "@Language.component(\"animal_component\")\n",
    "def animal_component_function(doc):\n",
    "    # Apply the matcher to the doc\n",
    "    matches = matcher(doc)\n",
    "    # Create a Span for each match and assign the label \"ANIMAL\"\n",
    "    spans = [Span(doc, start, end, label=\"ANIMAL\") for match_id, start, end in matches]\n",
    "    # Overwrite the doc.ents with the matched spans\n",
    "    doc.ents = spans\n",
    "    return doc\n",
    "\n",
    "# Add the component to the pipeline after the \"ner\" component\n",
    "nlp.add_pipe('animal_component', after=\"ner\")\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Process the txt and print the text and label for the doc.ents\n",
    "doc = nlp(\"I have a cat and a Golden Retriever\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov  4 2022, 16:35:55) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6309bee4636d05b1d5bc9eacc6d790f54642d13408168824efed5e6afed36196"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
