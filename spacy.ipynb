{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy operates on a pipeline which consists of many steps including tokenisation, lemmisation and other optional add in components.\n",
    "\n",
    "Pipeline is instantiated with a spacy pipeline by loading either a blank one with `spacy.blank(<language>)` or with `spacy.load(<name of trained model>)`\n",
    "\n",
    "Objects\n",
    "- nlp : pipeline\n",
    "- doc : the end result of the raw text through the pipeline\n",
    "- token : individual token (word or component) in the document\n",
    "- Span : Mulitple tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_string = \"Randy is learning NLP on spacy!\"\n",
    "doc =nlp(sample_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Randy is learning NLP on spacy!'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens\n",
    "tokens = [tok for tok in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Randy, is, learning, NLP, on, spacy, !]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Randy, is, learning]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spans\n",
    "sample_span = tokens[:3]\n",
    "sample_span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examining Randy in idx 0\n",
      "Alpha: True\n",
      "ascii: True\n",
      "Title case: True\n",
      "Lower case: False\n",
      "Left punctuation: False\n",
      "Right punctuation: False\n",
      "Number: False\n",
      "OOV?: True\n",
      "url: False\n",
      "Email: False\n",
      "Examining is in idx 1\n",
      "Alpha: True\n",
      "ascii: True\n",
      "Title case: False\n",
      "Lower case: True\n",
      "Left punctuation: False\n",
      "Right punctuation: False\n",
      "Number: False\n",
      "OOV?: True\n",
      "url: False\n",
      "Email: False\n",
      "Examining learning in idx 2\n",
      "Alpha: True\n",
      "ascii: True\n",
      "Title case: False\n",
      "Lower case: True\n",
      "Left punctuation: False\n",
      "Right punctuation: False\n",
      "Number: False\n",
      "OOV?: True\n",
      "url: False\n",
      "Email: False\n",
      "Examining NLP in idx 3\n",
      "Alpha: True\n",
      "ascii: True\n",
      "Title case: False\n",
      "Lower case: False\n",
      "Left punctuation: False\n",
      "Right punctuation: False\n",
      "Number: False\n",
      "OOV?: True\n",
      "url: False\n",
      "Email: False\n",
      "Examining on in idx 4\n",
      "Alpha: True\n",
      "ascii: True\n",
      "Title case: False\n",
      "Lower case: True\n",
      "Left punctuation: False\n",
      "Right punctuation: False\n",
      "Number: False\n",
      "OOV?: True\n",
      "url: False\n",
      "Email: False\n",
      "Examining spacy in idx 5\n",
      "Alpha: True\n",
      "ascii: True\n",
      "Title case: False\n",
      "Lower case: True\n",
      "Left punctuation: False\n",
      "Right punctuation: False\n",
      "Number: False\n",
      "OOV?: True\n",
      "url: False\n",
      "Email: False\n",
      "Examining ! in idx 6\n",
      "Alpha: False\n",
      "ascii: True\n",
      "Title case: False\n",
      "Lower case: False\n",
      "Left punctuation: False\n",
      "Right punctuation: False\n",
      "Number: False\n",
      "OOV?: True\n",
      "url: False\n",
      "Email: False\n"
     ]
    }
   ],
   "source": [
    "# Lexical attributes\n",
    "# Doc: https://spacy.io/api/token\n",
    "# e.g token.is_alpha , token.is_punct , token.like_num\n",
    "for idx, token in enumerate(tokens):\n",
    "    print(f\"Examining {token} in idx {idx}\")\n",
    "    print(f\"Alpha: {token.is_alpha}\")\n",
    "    print(f\"ascii: {token.is_ascii}\")\n",
    "    print(f\"Title case: {token.is_title}\")\n",
    "    print(f\"Lower case: {token.is_lower}\")\n",
    "    print(f\"Left punctuation: {token.is_left_punct}\")\n",
    "    print(f\"Right punctuation: {token.is_right_punct}\")\n",
    "    print(f\"Number: {token.like_num}\")\n",
    "    print(f\"OOV?: {token.is_oov}\")\n",
    "    print(f\"url: {token.like_url}\")\n",
    "    print(f\"Email: {token.like_email}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\tools\\miniconda3\\envs\\nlp-tutorials\\lib\\site-packages\\spacy\\util.py:877: UserWarning: [W095] Model 'en_core_web_sm' (3.3.0) was trained with spaCy v3.3 and may not be 100% compatible with the current version (3.4.3). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "nlp_en_core_web_sm = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[chngyuanlong@gmail.com]\n",
      "Like email : True\n"
     ]
    }
   ],
   "source": [
    "sample_email = \"chngyuanlong@gmail.com\"\n",
    "sample_url = \"https://myfirstdatasciencejob.wordpress.com/\"\n",
    "doc = nlp_en_core_web_sm(sample_email)\n",
    "tokens = [tok for tok in doc]\n",
    "print(tokens)\n",
    "print(f\"Like email : {tokens[0].like_email}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[https://myfirstdatasciencejob.wordpress.com/]\n",
      "Like url : True\n"
     ]
    }
   ],
   "source": [
    "doc = nlp_en_core_web_sm(sample_url)\n",
    "tokens = [tok for tok in doc]\n",
    "print(tokens)\n",
    "print(f\"Like url : {tokens[0].like_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[My, cat, is, like, a, god, ., Oh, opps, I, meant, dog]\n"
     ]
    }
   ],
   "source": [
    "sample_similarity_sentence = \"My cat is like a god. Oh opps I meant dog\"\n",
    "doc = nlp_en_core_web_sm(sample_similarity_sentence)\n",
    "tokens = [tok for tok in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34402838349342346\n",
      "0.23464104533195496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Randy\\AppData\\Local\\Temp\\ipykernel_24384\\2458266744.py:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  print(tokens[1].similarity(tokens[5]))\n",
      "C:\\Users\\Randy\\AppData\\Local\\Temp\\ipykernel_24384\\2458266744.py:2: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  print(tokens[1].similarity(tokens[-1]))\n"
     ]
    }
   ],
   "source": [
    "print(tokens[1].similarity(tokens[5]))\n",
    "print(tokens[1].similarity(tokens[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The thing that fascinates me is that some inbuilt features that relates to core NLP tasks that dependency parsing, NER, POS, identification of syntactic heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: My, POS Tag: PRON, Dep label: poss, Head token: cat, Lemmatised token: my\n",
      "Text: cat, POS Tag: NOUN, Dep label: nsubj, Head token: is, Lemmatised token: cat\n",
      "Text: is, POS Tag: AUX, Dep label: ROOT, Head token: is, Lemmatised token: be\n",
      "Text: like, POS Tag: ADP, Dep label: prep, Head token: is, Lemmatised token: like\n",
      "Text: a, POS Tag: DET, Dep label: det, Head token: god, Lemmatised token: a\n",
      "Text: god, POS Tag: NOUN, Dep label: pobj, Head token: like, Lemmatised token: god\n",
      "Text: ., POS Tag: PUNCT, Dep label: punct, Head token: is, Lemmatised token: .\n",
      "Text: Oh, POS Tag: INTJ, Dep label: poss, Head token: opps, Lemmatised token: oh\n",
      "Text: opps, POS Tag: NOUN, Dep label: npadvmod, Head token: meant, Lemmatised token: opps\n",
      "Text: I, POS Tag: PRON, Dep label: nsubj, Head token: meant, Lemmatised token: I\n",
      "Text: meant, POS Tag: VERB, Dep label: ROOT, Head token: meant, Lemmatised token: mean\n",
      "Text: dog, POS Tag: NOUN, Dep label: dobj, Head token: meant, Lemmatised token: dog\n"
     ]
    }
   ],
   "source": [
    "for token in tokens:\n",
    "    print(f\"Text: {token.text}, POS Tag: {token.pos_}, Dep label: {token.dep_}, Head token: {token.head.text}, Lemmatised token: {token.lemma_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there is no entity it will not print it.\n",
    "for ent in doc.ents:\n",
    "    # Print the entity text and label\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[We, will, need, to, destroy, the, Whiskey, company, else, they, get, too, big]\n"
     ]
    }
   ],
   "source": [
    "sample_NER = \"We will need to destroy the Whiskey company else they get too big\"\n",
    "doc = nlp_en_core_web_sm(sample_NER)\n",
    "tokens = [tok for tok in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whiskey ORG\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    # Print the entity text and label\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[From website](https://course.spacy.io/en/chapter1)\n",
    "\n",
    "Compared to regular expressions, the matcher works with Doc and Token objects instead of only strings.\n",
    "\n",
    "It's also more flexible: you can search for texts but also other lexical attributes.\n",
    "\n",
    "You can even write rules that use a model's predictions.\n",
    "\n",
    "For example, find the word \"duck\" only if it's a verb, not a noun."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! Important things to note: each dictionary pattern represents one token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_matcher_text = \"I need two dozen eggs and maybe 1kg of minced beef. Do not beef me\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp_en_core_web_sm(sample_matcher_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I PRON\n",
      "need VERB\n",
      "two NUM\n",
      "dozen NOUN\n",
      "eggs NOUN\n",
      "and CCONJ\n",
      "maybe ADV\n",
      "1 NUM\n",
      "kg NOUN\n",
      "of ADP\n",
      "minced VERB\n",
      "beef NOUN\n",
      ". PUNCT\n",
      "Do AUX\n",
      "not PART\n",
      "beef VERB\n",
      "me PRON\n"
     ]
    }
   ],
   "source": [
    "tokens = [tok for tok in doc]\n",
    "\n",
    "for token in tokens:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Matcher with the shared vocabulary\n",
    "matcher = Matcher(nlp_en_core_web_sm.vocab)\n",
    "\n",
    "# Let's try to catch the following pattern : VERB , [NOUN or PRON]\n",
    "VERB_PATTERN = [{\"POS\":\"VERB\"}, {\"POS\":{\"IN\":[\"PRON\", \"NOUN\"]}}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: ['minced beef', 'beef me']\n"
     ]
    }
   ],
   "source": [
    "# Add the pattern to the matcher\n",
    "matcher.add(\"items_pattern\", [VERB_PATTERN])\n",
    "\n",
    "# Use the matcher on the doc\n",
    "matches = matcher(doc)\n",
    "print(\"Matches:\", [doc[start:end].text for match_id, start, end in matches])\n",
    "\n",
    "# It fetches according to the POS tags but minced meat would be more like ADJ NOUN rather than VERB NOUN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StringStore, Lexemes and Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "doc = nlp(\"I have a cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_hash = nlp.vocab.strings['cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5439657043933447811"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_string = nlp.vocab.strings[cat_hash]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cat'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14692702688101715474"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab.strings['have']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'have'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab.strings[14692702688101715474]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3687079329867984377"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab.strings['fishy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"[E018] Can't retrieve string for hash '3687079329867984377'. This usually refers to an issue with the `Vocab` or `StringStore`.\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m nlp\u001b[39m.\u001b[39;49mvocab\u001b[39m.\u001b[39;49mstrings[\u001b[39m3687079329867984377\u001b[39;49m]\n",
      "File \u001b[1;32mc:\\tools\\miniconda3\\envs\\nlp-tutorials\\lib\\site-packages\\spacy\\strings.pyx:159\u001b[0m, in \u001b[0;36mspacy.strings.StringStore.__getitem__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"[E018] Can't retrieve string for hash '3687079329867984377'. This usually refers to an issue with the `Vocab` or `StringStore`.\""
     ]
    }
   ],
   "source": [
    "nlp.vocab.strings[3687079329867984377]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3687079329867984377"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab.strings.add('fishy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3687079329867984377"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab.strings['fishy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fishy'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab.strings[3687079329867984377]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexeme = nlp.vocab['fishy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3687079329867984377"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexeme.orth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Doc, Span manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc\n",
    "\n",
    "words = [\"spacy\",\"is\",'cool','!']\n",
    "spaces = [True, True, False, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spacy is cool!'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Doc, Span and Entities manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "words = [\"I\", \"like\", \"David\", \"Bowie\"]\n",
    "spaces = [True, True, True, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = Doc(nlp.vocab, words, spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I like David Bowie'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "span = Span(doc, 2,4, label=\"PERSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'David Bowie'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "380"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.ents = [span]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('David Bowie', 'PERSON')]\n"
     ]
    }
   ],
   "source": [
    "# Print entities' text and labels\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the following code that best use the features of Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\tools\\miniconda3\\envs\\nlp-tutorials\\lib\\site-packages\\spacy\\util.py:877: UserWarning: [W095] Model 'en_core_web_sm' (3.3.0) was trained with spaCy v3.3 and may not be 100% compatible with the current version (3.4.3). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found proper noun before a verb: Berlin\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Berlin looks like a nice city\")\n",
    "\n",
    "# Get all tokens and part-of-speech tags\n",
    "token_texts = [token.text for token in doc]\n",
    "pos_tags = [token.pos_ for token in doc]\n",
    "\n",
    "for index, pos in enumerate(pos_tags):\n",
    "    # Check if the current token is a proper noun\n",
    "    if pos == \"PROPN\":\n",
    "        # Check if the next token is a verb\n",
    "        if pos_tags[index + 1] == \"VERB\":\n",
    "            result = token_texts[index]\n",
    "            print(\"Found proper noun before a verb:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\tools\\miniconda3\\envs\\nlp-tutorials\\lib\\site-packages\\spacy\\util.py:877: UserWarning: [W095] Model 'en_core_web_sm' (3.3.0) was trained with spaCy v3.3 and may not be 100% compatible with the current version (3.4.3). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Singapore takes the cake everytime!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found proper noun before a verb: Singapore\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    if (token.pos_ == \"PROPN\") and doc[token.i + 1].pos_ == \"VERB\":\n",
    "        res = token.text\n",
    "        print(\"Found proper noun before a verb:\", res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6309bee4636d05b1d5bc9eacc6d790f54642d13408168824efed5e6afed36196"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
