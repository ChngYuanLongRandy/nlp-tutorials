{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy operates on a pipeline which consists of many steps including tokenisation, lemmisation and other optional add in components.\n",
    "\n",
    "Pipeline is instantiated with a spacy pipeline by loading either a blank one with `spacy.blank(<language>)` or with `spacy.load(<name of trained model>)`\n",
    "\n",
    "Objects\n",
    "- nlp : pipeline\n",
    "- doc : the end result of the raw text through the pipeline\n",
    "- token : individual token (word or component) in the document\n",
    "- Span : Mulitple tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_string = \"Randy is learning NLP on spacy!\"\n",
    "doc =nlp(sample_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Randy is learning NLP on spacy!'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens\n",
    "tokens = [tok for tok in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Randy, is, learning, NLP, on, spacy, !]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Randy, is, learning]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spans\n",
    "sample_span = tokens[:3]\n",
    "sample_span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examining Randy in idx 0\n",
      "Alpha: True\n",
      "ascii: True\n",
      "Title case: True\n",
      "Lower case: False\n",
      "Left punctuation: False\n",
      "Right punctuation: False\n",
      "Number: False\n",
      "OOV?: True\n",
      "url: False\n",
      "Email: False\n",
      "Examining is in idx 1\n",
      "Alpha: True\n",
      "ascii: True\n",
      "Title case: False\n",
      "Lower case: True\n",
      "Left punctuation: False\n",
      "Right punctuation: False\n",
      "Number: False\n",
      "OOV?: True\n",
      "url: False\n",
      "Email: False\n",
      "Examining learning in idx 2\n",
      "Alpha: True\n",
      "ascii: True\n",
      "Title case: False\n",
      "Lower case: True\n",
      "Left punctuation: False\n",
      "Right punctuation: False\n",
      "Number: False\n",
      "OOV?: True\n",
      "url: False\n",
      "Email: False\n",
      "Examining NLP in idx 3\n",
      "Alpha: True\n",
      "ascii: True\n",
      "Title case: False\n",
      "Lower case: False\n",
      "Left punctuation: False\n",
      "Right punctuation: False\n",
      "Number: False\n",
      "OOV?: True\n",
      "url: False\n",
      "Email: False\n",
      "Examining on in idx 4\n",
      "Alpha: True\n",
      "ascii: True\n",
      "Title case: False\n",
      "Lower case: True\n",
      "Left punctuation: False\n",
      "Right punctuation: False\n",
      "Number: False\n",
      "OOV?: True\n",
      "url: False\n",
      "Email: False\n",
      "Examining spacy in idx 5\n",
      "Alpha: True\n",
      "ascii: True\n",
      "Title case: False\n",
      "Lower case: True\n",
      "Left punctuation: False\n",
      "Right punctuation: False\n",
      "Number: False\n",
      "OOV?: True\n",
      "url: False\n",
      "Email: False\n",
      "Examining ! in idx 6\n",
      "Alpha: False\n",
      "ascii: True\n",
      "Title case: False\n",
      "Lower case: False\n",
      "Left punctuation: False\n",
      "Right punctuation: False\n",
      "Number: False\n",
      "OOV?: True\n",
      "url: False\n",
      "Email: False\n"
     ]
    }
   ],
   "source": [
    "# Lexical attributes\n",
    "# Doc: https://spacy.io/api/token\n",
    "# e.g token.is_alpha , token.is_punct , token.like_num\n",
    "for idx, token in enumerate(tokens):\n",
    "    print(f\"Examining {token} in idx {idx}\")\n",
    "    print(f\"Alpha: {token.is_alpha}\")\n",
    "    print(f\"ascii: {token.is_ascii}\")\n",
    "    print(f\"Title case: {token.is_title}\")\n",
    "    print(f\"Lower case: {token.is_lower}\")\n",
    "    print(f\"Left punctuation: {token.is_left_punct}\")\n",
    "    print(f\"Right punctuation: {token.is_right_punct}\")\n",
    "    print(f\"Number: {token.like_num}\")\n",
    "    print(f\"OOV?: {token.is_oov}\")\n",
    "    print(f\"url: {token.like_url}\")\n",
    "    print(f\"Email: {token.like_email}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\tools\\miniconda3\\envs\\nlp-tutorials\\lib\\site-packages\\spacy\\util.py:877: UserWarning: [W095] Model 'en_core_web_sm' (3.3.0) was trained with spaCy v3.3 and may not be 100% compatible with the current version (3.4.3). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "nlp_en_core_web_sm = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[chngyuanlong@gmail.com]\n",
      "Like email : True\n"
     ]
    }
   ],
   "source": [
    "sample_email = \"chngyuanlong@gmail.com\"\n",
    "sample_url = \"https://myfirstdatasciencejob.wordpress.com/\"\n",
    "doc = nlp_en_core_web_sm(sample_email)\n",
    "tokens = [tok for tok in doc]\n",
    "print(tokens)\n",
    "print(f\"Like email : {tokens[0].like_email}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[https://myfirstdatasciencejob.wordpress.com/]\n",
      "Like url : True\n"
     ]
    }
   ],
   "source": [
    "doc = nlp_en_core_web_sm(sample_url)\n",
    "tokens = [tok for tok in doc]\n",
    "print(tokens)\n",
    "print(f\"Like url : {tokens[0].like_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[My, cat, is, like, a, god, ., Oh, opps, I, meant, dog]\n"
     ]
    }
   ],
   "source": [
    "sample_similarity_sentence = \"My cat is like a god. Oh opps I meant dog\"\n",
    "doc = nlp_en_core_web_sm(sample_similarity_sentence)\n",
    "tokens = [tok for tok in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34402838349342346\n",
      "0.23464104533195496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Randy\\AppData\\Local\\Temp\\ipykernel_24384\\2458266744.py:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  print(tokens[1].similarity(tokens[5]))\n",
      "C:\\Users\\Randy\\AppData\\Local\\Temp\\ipykernel_24384\\2458266744.py:2: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  print(tokens[1].similarity(tokens[-1]))\n"
     ]
    }
   ],
   "source": [
    "print(tokens[1].similarity(tokens[5]))\n",
    "print(tokens[1].similarity(tokens[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The thing that fascinates me is that some inbuilt features that relates to core NLP tasks that dependency parsing, NER, POS, identification of syntactic heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: My, POS Tag: PRON, Dep label: poss, Head token: cat, Lemmatised token: my\n",
      "Text: cat, POS Tag: NOUN, Dep label: nsubj, Head token: is, Lemmatised token: cat\n",
      "Text: is, POS Tag: AUX, Dep label: ROOT, Head token: is, Lemmatised token: be\n",
      "Text: like, POS Tag: ADP, Dep label: prep, Head token: is, Lemmatised token: like\n",
      "Text: a, POS Tag: DET, Dep label: det, Head token: god, Lemmatised token: a\n",
      "Text: god, POS Tag: NOUN, Dep label: pobj, Head token: like, Lemmatised token: god\n",
      "Text: ., POS Tag: PUNCT, Dep label: punct, Head token: is, Lemmatised token: .\n",
      "Text: Oh, POS Tag: INTJ, Dep label: poss, Head token: opps, Lemmatised token: oh\n",
      "Text: opps, POS Tag: NOUN, Dep label: npadvmod, Head token: meant, Lemmatised token: opps\n",
      "Text: I, POS Tag: PRON, Dep label: nsubj, Head token: meant, Lemmatised token: I\n",
      "Text: meant, POS Tag: VERB, Dep label: ROOT, Head token: meant, Lemmatised token: mean\n",
      "Text: dog, POS Tag: NOUN, Dep label: dobj, Head token: meant, Lemmatised token: dog\n"
     ]
    }
   ],
   "source": [
    "for token in tokens:\n",
    "    print(f\"Text: {token.text}, POS Tag: {token.pos_}, Dep label: {token.dep_}, Head token: {token.head.text}, Lemmatised token: {token.lemma_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there is no entity it will not print it.\n",
    "for ent in doc.ents:\n",
    "    # Print the entity text and label\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[We, will, need, to, destroy, the, Whiskey, company, else, they, get, too, big]\n"
     ]
    }
   ],
   "source": [
    "sample_NER = \"We will need to destroy the Whiskey company else they get too big\"\n",
    "doc = nlp_en_core_web_sm(sample_NER)\n",
    "tokens = [tok for tok in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whiskey ORG\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    # Print the entity text and label\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[From website](https://course.spacy.io/en/chapter1)\n",
    "\n",
    "Compared to regular expressions, the matcher works with Doc and Token objects instead of only strings.\n",
    "\n",
    "It's also more flexible: you can search for texts but also other lexical attributes.\n",
    "\n",
    "You can even write rules that use a model's predictions.\n",
    "\n",
    "For example, find the word \"duck\" only if it's a verb, not a noun."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! Important things to note: each dictionary pattern represents one token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_matcher_text = \"I need two dozen eggs and maybe 1kg of minced beef. Do not beef me\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp_en_core_web_sm(sample_matcher_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I PRON\n",
      "need VERB\n",
      "two NUM\n",
      "dozen NOUN\n",
      "eggs NOUN\n",
      "and CCONJ\n",
      "maybe ADV\n",
      "1 NUM\n",
      "kg NOUN\n",
      "of ADP\n",
      "minced VERB\n",
      "beef NOUN\n",
      ". PUNCT\n",
      "Do AUX\n",
      "not PART\n",
      "beef VERB\n",
      "me PRON\n"
     ]
    }
   ],
   "source": [
    "tokens = [tok for tok in doc]\n",
    "\n",
    "for token in tokens:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Matcher with the shared vocabulary\n",
    "matcher = Matcher(nlp_en_core_web_sm.vocab)\n",
    "\n",
    "# Let's try to catch the following pattern : VERB , [NOUN or PRON]\n",
    "VERB_PATTERN = [{\"POS\":\"VERB\"}, {\"POS\":{\"IN\":[\"PRON\", \"NOUN\"]}}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: ['minced beef', 'beef me']\n"
     ]
    }
   ],
   "source": [
    "# Add the pattern to the matcher\n",
    "matcher.add(\"items_pattern\", [VERB_PATTERN])\n",
    "\n",
    "# Use the matcher on the doc\n",
    "matches = matcher(doc)\n",
    "print(\"Matches:\", [doc[start:end].text for match_id, start, end in matches])\n",
    "\n",
    "# It fetches according to the POS tags but minced meat would be more like ADJ NOUN rather than VERB NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6309bee4636d05b1d5bc9eacc6d790f54642d13408168824efed5e6afed36196"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
